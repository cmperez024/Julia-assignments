{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all datasets excluding the third, the madelon dataset, partitioning the data into training and test sets involves the use of the MLJ function, partition. This function, when passing in a range of numbers, generates two subsets of that range that contain randomly chosen numbers, and each set is mutually exclusive. For each dataset, partitioning is done in a 0.75-0.25 ratio. The returned values are train and test, which correspond to the indices of the subsets. We can then index by these returned values to get all the rows that correspond to these sets, named accordingly.\n",
    "\n",
    "For the partition function, a random seed was implemented to ensure consistency between results for debugging. The seed value can be adjusted freely, and can be removed from the function (the \"rng = rng\" part) if desired.\n",
    "\n",
    "We only need to partition the data once before feeding them into the models, and this process is roughly identical even between datasets; thus the variable names are roughly the same. Caution must be exercised when running models for other datasets: the code cell directly under \"Dataset N: \" must be ran first before applying any of the subsequent models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters were tuned automatically by the ScikitLearn package. To make predictions more accurate, however, simplifications often had to be made on a case-by-case basis. For instance, in datasets 2 and 4, certain classes were grouped together to allow for more accurate predictions. How this occurs will be explained more thoroughly in comments embedded in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulated Results\n",
    "\n",
    "\n",
    "**Important Note:** These values are relative to my machine and are presented for convenience. Thus, all values are subject to change, especially training and testing time. Accuracy is expected to be roughly similar due to setting a random seed.\n",
    "\n",
    "When running a code chunk for a model, training time, testing time, accuracy, and model size will be displayed under the chunk, particular to the machine running it.\n",
    "\n",
    "\n",
    "\n",
    "Training Time (seconds)\n",
    "\n",
    "| Dataset | Classification Tree | Naive Bayes | Support Vector Machine | Neural Network \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| Car Evaluation | 0.003052 | 0.006951 | 0.053788  | 0.908673 |\n",
    "| Abalone | 0.020845 | 0.011424 | 0.105136 |  1.750691 | \n",
    "| Madelon | 0.391014 | 0.014903 | 0.616664 |  0.399532 | \n",
    "| KDD Cup | 12.201162 | 12.587327 | 70.719620 | 177.687514 | \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Testing Time (seconds)\n",
    "\n",
    "| Dataset | Classification Tree | Naive Bayes | Support Vector Machine | Neural Network \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| Car Evaluation | 0.000720 | 0.000942 | 0.023795 | 0.001075 |\n",
    "| Abalone | 0.003780 | 0.003791 | 0.003925 | 0.004368 | \n",
    "| Madelon | 0.001105 | 0.004305 | 0.292484 | 0.002390 | \n",
    "| KDD Cup | 2.920390 | 3.126324 | 3.197580 | 3.299938 | \n",
    "\n",
    "\n",
    "Accuracy (Truncated to 3 Decimals)\n",
    "\n",
    "| Dataset | Classification Tree | Naive Bayes | Support Vector Machine | Neural Network \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| Car Evaluation | 0.967 | 0.884 | 0.974 | 0.995 |\n",
    "| Abalone | 0.547 | 0.577 | 0.651 | 0.660 | \n",
    "| Madelon | 0.731 | 0.591 | 0.686 | 0.538 | \n",
    "| KDD Cup | 0.999 | 0.978 | 0.990 | 0.997 | \n",
    "\n",
    "\n",
    "\n",
    "Model Size (Kilobytes)\n",
    "\n",
    "| Dataset | Classification Tree | Naive Bayes | Support Vector Machine | Neural Network \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| Car Evaluation | 17.712  | 5.341 | 142.076  | 92.098 |\n",
    "| Abalone | 105.545 | 1.101 | 0.961 | 105.545 | \n",
    "| Madelon |23.131 | 16.696 | 7301.657 | 1611.615 | \n",
    "| KDD Cup | 20.967 | 2.005 | 1.071 | 144.074 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, we must import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `C:\\Users\\rabbl\\.julia\\registries\\General`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\rabbl\\OneDrive - University of Florida\\Spring 2021\\Intro to Data Science\\code\\Manifest.toml`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <module 'joblib' from 'C:\\\\Users\\\\rabbl\\\\.julia\\\\conda\\\\3\\\\lib\\\\site-packages\\\\joblib\\\\__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"StableRNGs\")\n",
    "Pkg.add(\"MLJ\")\n",
    "Pkg.add(\"ScikitLearn\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"PyCall\")\n",
    "Pkg.add(\"Random\")\n",
    "\n",
    "using StableRNGs\n",
    "using MLJ:partition\n",
    "using ScikitLearn\n",
    "using CSV\n",
    "using DataFrames\n",
    "using PyCall:pyimport\n",
    "using Random\n",
    "\n",
    "\n",
    "# For dumping files to get size\n",
    "joblib = pyimport(\"joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1: Car Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is relatively straight forward to work with. The data is clumped together, so we must partition the data into training and testing sets ourselves. Since it contains strings as some variables, we must also encode them to integers to be accepted into the model. No further processing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432×21 Array{Float64,2}:\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  …  0.0  0.0  1.0  0.0  0.0  1.0  0.0\n",
       " 1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     1.0  0.0  1.0  0.0  0.0  1.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  1.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0     1.0  0.0  0.0  1.0  0.0  0.0  1.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  1.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  …  0.0  0.0  1.0  0.0  0.0  0.0  1.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     1.0  0.0  1.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  1.0  0.0  1.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0     1.0  0.0  1.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  …  0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     1.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
       " ⋮                        ⋮              ⋱       ⋮                        ⋮\n",
       " 0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  …  0.0  0.0  1.0  0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0     0.0  0.0  1.0  0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     1.0  0.0  1.0  0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0     1.0  0.0  0.0  1.0  0.0  0.0  1.0\n",
       " 0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  …  0.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0     1.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  1.0  0.0  0.0  0.0  0.0  1.0\n",
       " 1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  …  0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     1.0  0.0  1.0  0.0  0.0  1.0  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Data and separate it into X (feature) and Y (class) data\n",
    "# We also need to cinvert the dataframe into an array to that the model will accept it\n",
    "df = CSV.File(\"Dataset1/car.data\", header = false) |> DataFrame\n",
    "carX = convert(Array, df[:, [1, 2, 3, 4, 5, 6]])\n",
    "carY = convert(Array, df[:, 7])\n",
    "\n",
    "# Generate the partition indices for training and subset data\n",
    "rng = StableRNG(566)\n",
    "train, test = partition(1:length(carY), 0.75, shuffle = true, rng = rng)\n",
    "\n",
    "# Define the training and testing subsets\n",
    "trainX = carX[train, :]\n",
    "trainY = carY[train, :]\n",
    "testX = carX[test, :]\n",
    "testY = carY[test, :]\n",
    "\n",
    "# Being that carX contains categorical (namely non-integer) data,\n",
    "# we must encode it so that the model accepts it.\n",
    "@sk_import preprocessing: OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "\n",
    "# Fit the encoder to the whole x data\n",
    "enc.fit(carX)\n",
    "\n",
    "# Encode the train and test subsets using the fitted encoder\n",
    "trainX = enc.transform(trainX).toarray()\n",
    "testX = enc.transform(testX).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.003052 seconds (2.61 k allocations: 41.344 KiB)\n",
      "Testing time:  0.000720 seconds (2.65 k allocations: 77.656 KiB)\n",
      "Proportion correct: 0.9675925925925926\n",
      "Model size: 17.712 KB"
     ]
    }
   ],
   "source": [
    "# Import the model\n",
    "@sk_import  tree: DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# We now fit the model to the training data and time this process\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(tree_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "# With the fitted mode, we make predictions and compare it to the testY, \n",
    "# to see if the model was correct. Get the proportion of correct predictions.\n",
    "# We also time this process\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(tree_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# To get the model size we dump the variable into the local directory\n",
    "# using a python module, then check the size using base julia.\n",
    "joblib.dump(tree_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The process above for importing, training, testing, and modeling will be identical for each model within each data set. Thus, future comments will not be as comprehensive for the modeling code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.005423 seconds (2.61 k allocations: 41.344 KiB)\n",
      "Testing time:  0.001064 seconds (2.65 k allocations: 77.656 KiB)\n",
      "Proportion correct: 0.8842592592592593\n",
      "Model size: 5.341 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  naive_bayes: CategoricalNB\n",
    "bayes_model = CategoricalNB()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(bayes_model, trainX, trainY)\n",
    "end\n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(bayes_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(bayes_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.053105 seconds (2.61 k allocations: 41.344 KiB)\n",
      "Testing time:  0.024272 seconds (2.65 k allocations: 77.656 KiB)\n",
      "Proportion correct: 0.9745370370370371\n",
      "Model size: 142.076 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  svm: SVC\n",
    "support_model = SVC()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(support_model, trainX, trainY)\n",
    "end\n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(support_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(support_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  1.069224 seconds (302.91 k allocations: 14.971 MiB)\n",
      "Testing time:  0.247096 seconds (653.91 k allocations: 33.713 MiB, 4.02% gc time)\n",
      "Proportion correct: 0.9976851851851852\n",
      "Model size: 92.098 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import neural_network: MLPClassifier\n",
    "nn_model = MLPClassifier()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(nn_model, trainX, trainY)\n",
    "end\n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(nn_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(nn_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2: Abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset had only one string column, so instead of using OneHotEncoder (which is not great for high dimensional data), we use label encoder to manually encode certain columns.\n",
    "\n",
    "The quirk about this dataset is that there are 29 classes to predict from. Thus, to make the problem more sensible, we group the classes together based on the median. This was done in spirit of a paper mentioned in the abalone.names data. We will group classes 1-8, 9-10, and 11-29 together as 3 distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044×8 Array{Any,2}:\n",
       " 2  0.595  0.465  0.175  1.115   0.4015  0.254   0.39\n",
       " 2  0.62   0.495  0.18   1.2555  0.5765  0.254   0.355\n",
       " 2  0.57   0.465  0.125  0.849   0.3785  0.1765  0.24\n",
       " 1  0.275  0.2    0.065  0.1035  0.0475  0.0205  0.03\n",
       " 1  0.485  0.375  0.13   0.6025  0.2935  0.1285  0.16\n",
       " 2  0.505  0.4    0.125  0.77    0.2735  0.159   0.255\n",
       " 2  0.62   0.49   0.19   1.218   0.5455  0.2965  0.355\n",
       " 2  0.76   0.605  0.215  2.173   0.801   0.4915  0.646\n",
       " 2  0.545  0.425  0.135  0.8445  0.373   0.21    0.235\n",
       " 1  0.28   0.215  0.07   0.124   0.063   0.0215  0.03\n",
       " 2  0.7    0.55   0.2    1.523   0.693   0.306   0.4405\n",
       " 0  0.59   0.44   0.14   1.007   0.4775  0.2105  0.2925\n",
       " 1  0.335  0.26   0.1    0.192   0.0785  0.0585  0.07\n",
       " ⋮                               ⋮               \n",
       " 1  0.545  0.43   0.15   0.742   0.3525  0.158   0.208\n",
       " 0  0.58   0.45   0.15   0.92    0.393   0.212   0.2895\n",
       " 1  0.44   0.34   0.12   0.438   0.2115  0.083   0.12\n",
       " 0  0.55   0.405  0.125  0.651   0.2965  0.137   0.2\n",
       " 2  0.465  0.35   0.12   0.5205  0.2015  0.1625  0.185\n",
       " 2  0.655  0.52   0.17   1.1445  0.53    0.223   0.348\n",
       " 2  0.5    0.39   0.135  0.6595  0.3145  0.1535  0.1565\n",
       " 1  0.425  0.31   0.095  0.3075  0.139   0.0745  0.093\n",
       " 1  0.365  0.27   0.085  0.1875  0.081   0.042   0.058\n",
       " 2  0.645  0.515  0.175  1.6115  0.6745  0.384   0.385\n",
       " 1  0.25   0.185  0.065  0.0685  0.0295  0.014   0.0225\n",
       " 2  0.635  0.475  0.17   1.1935  0.5205  0.2695  0.3665"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data, get the X and Y. Convert to arrays\n",
    "df = CSV.File(\"Dataset2/abalone.data\", header = false) |> DataFrame\n",
    "abaX = convert(Array, df[:, 1:8])\n",
    "abaY = convert(Array, df[:, 9])\n",
    "\n",
    "# Transform y into 3 ranges based on ring number\n",
    "# class 1-8: 0\n",
    "# class 9-10: 1\n",
    "# class 11-29: 2\n",
    "\n",
    "# For all values less than or equal to 8, turn them into a 0\n",
    "abaY[abaY .<= 8] .= 0\n",
    "\n",
    "# For all values that are 9 or 10, turn them into a 1\n",
    "abaY[abaY .== 9] .= 1\n",
    "abaY[abaY .== 10] .= 1\n",
    "\n",
    "# For all values greater than or equal to 11, turn them into a 2\n",
    "abaY[abaY .>= 11] .= 2\n",
    "\n",
    "# Now we generate the partitioned indices\n",
    "rng = StableRNG(566)\n",
    "train, test = partition(1:length(abaY), 0.75, shuffle = true, rng = rng)\n",
    "\n",
    "#  Get the training data for Y. We need to encode the X data first, so we generate\n",
    "# trainX and testX later.\n",
    "trainY = abaY[train, :]\n",
    "testY = abaY[test, :]\n",
    "\n",
    "# Load the encoder and fit it to the whole X data\n",
    "@sk_import preprocessing: LabelEncoder\n",
    "lenc = LabelEncoder()\n",
    "lenc.fit(abaX[:, 1])\n",
    "\n",
    "# Encode the 1st column which contains string data\n",
    "abaX[:, 1] = lenc.transform(abaX[:, 1])\n",
    "\n",
    "# Subset the remaining train and test data for X\n",
    "trainX = abaX[train, :]\n",
    "testX = abaX[test, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.022262 seconds (56.41 k allocations: 882.031 KiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant DecisionTreeRegressor. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time:  0.003497 seconds (18.83 k allocations: 307.922 KiB)\n",
      "Proportion correct: 0.5660919540229885\n",
      "Model size: 106.185 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  tree: DecisionTreeRegressor\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(tree_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(tree_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(tree_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.012057 seconds (56.41 k allocations: 882.031 KiB)\n",
      "Testing time:  0.249947 seconds (1.03 M allocations: 48.094 MiB, 4.56% gc time)\n",
      "Proportion correct: 0.5775862068965517\n",
      "Model size: 1.101 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  naive_bayes: GaussianNB\n",
    "bayes_model = GaussianNB()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(bayes_model, trainX, trainY)\n",
    "end\n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(bayes_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(bayes_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant LinearSVC. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.091146 seconds (56.41 k allocations: 882.031 KiB)\n",
      "Testing time:  0.003598 seconds (18.85 k allocations: 309.078 KiB)\n",
      "Proportion correct: 0.6513409961685823\n",
      "Model size: 0.961 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  svm: LinearSVC\n",
    "support_model =LinearSVC()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(support_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(support_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(support_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant MLPClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  1.816154 seconds (56.41 k allocations: 882.031 KiB)\n",
      "Testing time:  0.004466 seconds (18.85 k allocations: 309.078 KiB)\n",
      "Proportion correct: 0.6590038314176245\n",
      "Model size: 106.185 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import neural_network: MLPClassifier\n",
    "nn_model = MLPClassifier()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(nn_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(nn_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(tree_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3: Madelon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this dataset is particularly nice to us since they already partitioned the data into training and testing data (which were called validation data in this case). Additionally, since all features were numeric, we do not need to perform any encoding.\n",
    "\n",
    "However, we should still shuffle the data to induce some randomness, so we do that using the shuffle function from the Random package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600×1 Array{Int64,2}:\n",
       " -1\n",
       " -1\n",
       "  1\n",
       " -1\n",
       " -1\n",
       " -1\n",
       "  1\n",
       "  1\n",
       " -1\n",
       " -1\n",
       "  1\n",
       " -1\n",
       "  1\n",
       "  ⋮\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " -1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " -1\n",
       "  1\n",
       " -1\n",
       " -1\n",
       " -1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data. It is already partitioned\n",
    "dfTrain = CSV.File(\"Dataset3/madelon_train.data\", header = false) |> DataFrame\n",
    "dfTest = CSV.File(\"Dataset3/madelon_test.data\", header = false) |> DataFrame\n",
    "dfValid = CSV.File(\"Dataset3/madelon_valid.data\", header = false) |> DataFrame\n",
    "\n",
    "dfTrainLabels = CSV.File(\"Dataset3/madelon_train.labels\", header = false) |> DataFrame\n",
    "dfValidLabels = CSV.File(\"Dataset3/madelon_valid.labels\", header = false) |> DataFrame\n",
    "\n",
    "# Get the x and y data, while converting to arrays\n",
    "trainX = convert(Array, dfTrain[:, 1:500])\n",
    "trainY = convert(Array, dfTrainLabels)\n",
    "\n",
    "testX = convert(Array, dfValid[:, 1:500])\n",
    "testY = convert(Array, dfValidLabels)\n",
    "\n",
    "# Shuffle the data. Generate shuffled indices then index the data\n",
    "rng = StableRNG(566)\n",
    "trainShuffle = Random.shuffle(rng, 1:length(trainY))\n",
    "testShuffle = Random.shuffle(rng, 1:length(testY))\n",
    "\n",
    "# Now we index by the shuffled indices to get the same data, albeit shuffed\n",
    "trainX = trainX[trainShuffle, :]\n",
    "trainY = trainY[trainShuffle, :]\n",
    "\n",
    "testX = testX[testShuffle, :]\n",
    "testY = testY[testShuffle, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant DecisionTreeClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.391014 seconds (22 allocations: 1.125 KiB)\n",
      "Testing time:  0.001105 seconds (62 allocations: 12.141 KiB)\n",
      "Proportion correct: 0.7316666666666667\n",
      "Model size: 23.131 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  tree: DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(tree_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(tree_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(tree_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant GaussianNB. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.014903 seconds (22 allocations: 1.125 KiB)\n",
      "Testing time:  0.004305 seconds (62 allocations: 12.141 KiB)\n",
      "Proportion correct: 0.5916666666666667\n",
      "Model size: 16.696 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  naive_bayes: GaussianNB\n",
    "bayes_model =  GaussianNB()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(bayes_model, trainX, trainY)\n",
    "end\n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(bayes_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(bayes_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.616664 seconds (22 allocations: 1.125 KiB)\n",
      "Testing time:  0.292484 seconds (62 allocations: 12.141 KiB)\n",
      "Proportion correct: 0.6866666666666666\n",
      "Model size: 7301.657 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  svm: SVC\n",
    "support_model = SVC()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(support_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(support_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(support_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant MLPClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.399532 seconds (22 allocations: 1.125 KiB)\n",
      "Testing time:  0.002390 seconds (62 allocations: 12.141 KiB)\n",
      "Proportion correct: 0.5383333333333333\n",
      "Model size: 1611.615 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import neural_network: MLPClassifier\n",
    "nn_model = MLPClassifier()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(nn_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(nn_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(nn_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 4: KDD Cup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is extremely large, and for computational ease, we are only using the 10% data, which still contains roughly 400 thousand rows. It also has about 42 columns, meaning this data has very high dimensionality. Combined with the fact that it has string data, we opt to use label encoding (instead of OneHot) so that dimensionality isn't increased.\n",
    "\n",
    "Much like the Abalone dataset, there are many classes. In the context of this dataset, a class variable is either known as \"normal,\" or an \"attack,\" of which there are several types of the latter. We reduce the data into just \"normal\" or \"attack\" classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant LabelEncoder. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123505×1 Array{Int64,2}:\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and convert to arrays\n",
    "df = CSV.File(\"Dataset4/kddcup.data_10_percent_corrected\", header = false) |> DataFrame\n",
    "kddX = convert(Array, df[:, 1:41])\n",
    "kddY = convert(Array, df[:, 42])\n",
    "\n",
    "\n",
    "# There are several attack types. must map normal to normal and all others to something else\n",
    "# Convert to binary classification. not-attack vs. attack\n",
    "kddY[kddY .== \"normal.\"] .= \"0\"\n",
    "kddY[kddY .!= \"0\"] .= \"1\"\n",
    "\n",
    "# Being that they were strings, we now parse the data into integers to push into the model\n",
    "kddY = parse.(Int64, kddY)\n",
    "\n",
    "\n",
    "# However, there are still string columns remaining, so we must encode them\n",
    "# There are 3 of them precisely.\n",
    "@sk_import preprocessing: LabelEncoder\n",
    "lenc2 = LabelEncoder()\n",
    "lenc3 = LabelEncoder()\n",
    "lenc4 = LabelEncoder()\n",
    "\n",
    "lenc2.fit(kddX[:, 2])\n",
    "kddX[:, 2] = lenc2.transform(kddX[:, 2])\n",
    "\n",
    "lenc3.fit(kddX[:, 3])\n",
    "kddX[:, 3] = lenc3.transform(kddX[:, 3])\n",
    "\n",
    "lenc4.fit(kddX[:, 4])\n",
    "kddX[:, 4] = lenc4.transform(kddX[:, 4])\n",
    "\n",
    "\n",
    "# Now we partition the data into training and test sets like as done before\n",
    "rng = StableRNG(566)\n",
    "\n",
    "train, test = partition(1:length(kddY), 0.75, shuffle = true, rng = rng)\n",
    "\n",
    "trainX = kddX[train, :]\n",
    "trainY = kddY[train, :]\n",
    "\n",
    "testX = kddX[test, :]\n",
    "testY = kddY[test, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant DecisionTreeClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 12.201162 seconds (31.12 M allocations: 474.906 MiB, 7.79% gc time)\n",
      "Testing time:  2.920390 seconds (10.37 M allocations: 159.265 MiB)\n",
      "Proportion correct: 0.9997166106635359\n",
      "Model size: 20.967 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  tree: DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(tree_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(tree_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(tree_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant GaussianNB. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 12.587327 seconds (31.12 M allocations: 474.905 MiB, 12.08% gc time)\n",
      "Testing time:  3.126324 seconds (10.37 M allocations: 159.265 MiB)\n",
      "Proportion correct: 0.9788510586615926\n",
      "Model size: 2.005 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  naive_bayes: GaussianNB\n",
    "bayes_model = GaussianNB()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(bayes_model, trainX, trainY)\n",
    "end\n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(bayes_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(bayes_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant LinearSVC. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 70.719620 seconds (31.12 M allocations: 474.905 MiB, 1.08% gc time)\n",
      "Testing time:  3.197580 seconds (10.37 M allocations: 159.265 MiB)\n",
      "Proportion correct: 0.9900408890328327\n",
      "Model size: 1.071 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import  svm: LinearSVC\n",
    "support_model = LinearSVC()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(support_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(support_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(support_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant MLPClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:177.687514 seconds (31.12 M allocations: 474.905 MiB, 0.44% gc time)\n",
      "Testing time:  3.299938 seconds (10.37 M allocations: 159.265 MiB)\n",
      "Proportion correct: 0.9975871422209627\n",
      "Model size: 144.074 KB"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "@sk_import neural_network: MLPClassifier\n",
    "nn_model = MLPClassifier()\n",
    "\n",
    "# Training and evaluation\n",
    "print(\"Training time:\")\n",
    "@time begin\n",
    "fit!(nn_model, trainX, trainY)\n",
    "end \n",
    "\n",
    "print(\"Testing time:\")\n",
    "@time begin\n",
    "accuracy = sum(predict(nn_model, testX) .== testY) / length(testY)\n",
    "end\n",
    "\n",
    "println(\"Proportion correct: \", accuracy)\n",
    "\n",
    "# Model size\n",
    "joblib.dump(nn_model, \"model\")\n",
    "sz = stat(\"model\").size\n",
    "print(\"Model size: $(sz/1000) KB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
